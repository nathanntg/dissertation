\chapter{Interface and software for near-real-time processing and feedback}
\label{chapter:processing}

\thispagestyle{myheadings}

% path to figures for this chapter
% must have a trailing slash
\graphicspath{{6_Processing/Figures/}}

\section{Introduction}

% TODO: introduction

\section{Methods}

% TODO: introduce methods section

Animal care and experimental procedures were 
approved by the Institutional Animal Care and Use Committee (IACUC) 
of Boston University (protocols 14-028 and 14-029). Fibers were 
implanted in nine zebra finches. Of those, we chronically recorded 
from seven birds via the fiber interface described in the next section; 
two were excluded due to issues with early design iterations, 
where the birds risked getting tangled in the fiber bundle.

\subsection{Chronic fiber interface}

To use the bundles of optical microfibers to interface 
with awake behaving animals, we have built and piloted 
a configuration where the animal is housed in an enclosure 
below a traditional fluorescence microscope with the 
ensheathed bundle running from the microscope to 
the animal. The bundle is secured to the animal using 
anchor points around the craniotomy. To avoid physical 
stress on the optical fibers, the bundle is reinforced 
with a strip of polyimide (DuPont Kapton, 0.005'' thick),
prepared as shown in \fref{fig:strip}. The strip is 
similarly runs from the skull implant points to the 
fluorescence microscope above the enclosure.

\begin{figure}
\includegraphics[width=\textwidth]{fig1-strip.eps}
\caption[Polyimide to reinforce fiber bundle]{\textbf{A diagram
of a prepared polyimide strip to reinforce a chronic fiber 
bundle implant.} A strip of polyimide is used to absorb physical 
stresses, protecting the bundle of fibers. The fiber bundle is 
threaded through the small holes and the bottom hole (right) is 
used to anchor the polyimide strip to the skull.}
\label{fig:strip}
\end{figure}

The polyimide strip is cut slightly shorter than the fiber 
bundle, so that when the animal moves or twist, the force 
will be primarily exerted on the strip rather than the bundle.
To secure the fiber bundle to the polyimide strip, we 
punched small holes in the strip, through which the 
fiber bundle was threaded. This ensured that the slack 
in the fiber bundle did not dangle or interfere with 
the animals behavior.

Depending on the duration of the recording session, the 
fiber can either be fixed below the fluorescent microscope 
or can be mounted in a low-friction passive commutator,
constructed from precision ball bearings (\fref{fig:commutator}), that allows 
the fiber bundle to rotate as the animal moves. Given the 
flexibility of the fibers, the polyimide reinforcement 
strip is beneficial here to translate the rotational 
torque up to the commutator.

\begin{figure}
\includegraphics[width=\textwidth]{fig2-commutator.jpg}
\caption[Commutator for awake behaving recording]{\textbf{Commutator
build with low-friction, precision ball bearings allows fiber to 
rotate.} A passive commutator constructed with precision ball 
bearings holds the ferrule and polished imaging surface below 
a microscope objective. As the animal moves about the enclosure,
the fiber can freely rotate, while remaining level in the focal 
plane of the microscope.}
\label{fig:commutator}
\end{figure}

In situations requiring longer acquisition exposure, the 
rotation of the fiber may be problematic. We tested a 
further variation that used a servo to apply a rubber pad 
to the passive commutator during recording, temporarily 
preventing rotation.

\subsection{Acquisition hardware}

% TODO: describe acquisition hardware

\subsection{Acquisition software}

% TODO: describe two software acquisition approaches

\subsection{Near-real-time software}

To manage and trigger acquisition, and to do near-real-time signal
processing, we created custom image acquisition software that runs
on the macOS operating system (\fref{fig:software}). Leveraging the 
native AVFoundation
framework, the software is capable of communicating with a wide 
range of USB video 
acquisition hardware, including both analog-to-digital frame 
grabbers and cameras. The software can capture video or video 
synchronized with multi-channel analog data (including 
either audio or other analog data). Video and analog data are 
written to disk in MPEG-4 container files with video encoded at 
full resolution either uncompressed or using the H.264 or lossless 
MJPEG Open DML codecs and audio encoded using either the AAC codec 
or the linear PCM format with a sampling rate of up to 
96~\si{\kilo\hertz}. Simultaneously, the software writes metadata 
including frame timing, region of interest intensity and analysis  
parameters to a text file (CSV format).

\begin{figure}
\includegraphics[width=\textwidth]{fig2-software.png}
\caption[Software for capture near-real-time analysis]{\textbf{User interface of acquisition
control software capable of near-real-time analysis.} The software 
controls capture related peripherals, including USB camera sensors,
light sources and microcontroller breakout boxes, and manages 
synchronized acquisition of video and analog signals. During 
acquisition, the software can process video frames, extracting 
multiple annotated regions of interest, and can process analog 
signals, such as applying audio template matching. Analysis 
can trigger external feedback, acting as a basic brain machine 
interface.}
\label{fig:software}
\end{figure}

The software also communicates with a microcontroller 
(Arduino Uno Rev3 or 2560 Mega) via a USB-to-serial connection. The 
microcontroller allows the software to control additional 
peripherals, including light source brightness, camera parameters 
(such as exposure) and the commutator.

The software is able to perform near real-time analysis on both the 
video frames and the analog input channels---these channels enable
low-latency, video-aligned acquisition of audio, external TTL 
pulses, electrophysiology or behavioral data. 
For example, in experiments we have run recording fluorescence in songbirds, 
we use the analog channels to acquire audio and trigger acquisition 
based off song detection (by processing audio through a short-time
Fourier transform to calculate the ratio of power in frequency 
bins associated with song [1--10~kHz] to the power of frequency bins outside 
the song range). The microcontroller is used to activate 
the excitation light source, restricting illumination to periods of 
singing and, as a result, restricting photobleaching. 

During monitoring and acquisition, the software can perform low-latency
computational analysis of both the video frames and analog input. In 
the case of the video, the user interface provides the ability to 
draw regions of interest (ROIs) onto the field of view and processing 
rules in the form of symbolic equations that can be used to trigger 
external feedback (e.g., sound or reward). At the start of
acquisition, the software converts the regions of interest into indices 
corresponding with the ROI masks and parses the symbolic equations 
into an abstract syntax tree (AST) for optimization and evaluation. 
Each frame is read in YCbCr color 
space, providing direct access to the luma component (Y). Using vector
optimized instructions when available, the software accumulates average 
pixel intensity for each region of interest and then evaluates the 
syntax tree and outputs the appropriate feedback value. \Fref{fig:software} 
shows the software configured to compare $\Delta$F values for two 
regions of interest corresponding with two fibers.

Given the broader range of analog analysis and applications, 
the software does not have a graphical 
user interface for defining analysis, but the code provides clear 
extension points. We have extended and open sourced versions 
of the software that use the analog signals to acquire and process 
audio. Acquired audio is converted to a spectral representation 
through a two-taper short-time Fourier transform (STFT). These 
analysis steps leverage the vDSP Accelerate framework, which 
utilizes vectorized instructions whenever possible to achieve 
low latency processing. We use the spectral representation to 
trigger acquisition (during singing) and to provide behavioral
feedback. The spectral 
representation can serve as an input to a neural network trained 
to precisely identify salient audio (i.e., a syllable or other 
vocalization) \cite{Pearre:2017cs}; upon detection, the software 
can provide aversive feedback (such as burst of white noise) 
to perturb behavior.

To evaluate the software latency, we blink an LED in the 
field of view of the camera, while recording the output trigger 
controlled by the software to detect when the LED intensity is 
detectable. The LED activation triggers activation on an 
oscilloscope, which is to averaging mode (128-trigger average) 
in order to collect latency data. In addition, external logging
flags in the software are used to evaluate processing time for 
specific steps in the analysis pipeline (such as ROI extraction).
Audio processing is evaluated as described in \cite{Pearre:2017cs},
using recordings of songs with a ground truth annotation on the 
second channel. The ground truth signal triggers oscilloscope 
acquisition, again averaging over 128 trials to build a distribution 
of timing performance.

\subsection{Analysis pipeline}

% TODO: describe node based analysis pipeline

\section{Results}

The chronic fiber interface coupled and associated video 
acquisition and analysis software were used to record 
from seven zebra finches for 7--23 days each (mean: 
17~days). The recording results are not 
presented here, but the sessions provided an opportunity to 
evaluate and revise the fiber interface for longitudinal 
awake behaving recordings.

\subsection{Chronic fiber interface}

The implanted fibers remained well secured over the 
course of the recording, and the setup provided the 
necessary freedom of motion to allow the birds to 
sing. For all five animals, we recorded fluorescence 
via the fiber and synchronized audio for hundreds 
of song renditions per day.

After 23 days implanted in an awake behaving animal,
we remeasured the optical attenuation. The optical 
attenuation of the used fibers was measured to be 
6.54~dB (compared with 3.78~dB pre-recording, as 
measured in \sref{sec:methods-fibers}).

\subsection{Analysis performance}

To evaluate the near-real-time analysis and feedback, we recorded 
signals that provide a reference signal that represents the 
ground truth. 

% TODO: potentially remeasure and generate new figure
Region of interest (ROI) video analysis was evaluated by measuring the
time from LED onset to software feedback. The average time to trigger 
a feedback stimulus is 23.9 $\pm$ 3.9~ms (std. dev.), 
including frame exposure, digitization, transmission, data acquisition 
and software processing. This latency reflects the intrinsic 
limitations of a 33~ms exposure time (which accounts for 16.5~ms of 
the delay) as well as the time spent capturing and decoding the video, 
and communicating an output TTL via the microcontroller. 
\Fref{fig:video-latency} depicts video controller latency.

\begin{figure}
\includegraphics[width=\textwidth]{fig3-video-latency.eps}
\caption[Latency for video processing]{\textbf{Video analysis 
produces feedback in 24~ms, including exposure time.} To measure 
the latency of the video analysis functionality in the 
acquisition software, an LED was flashed in the field of view,
while timing the delay until the output TTL indicating detection. 
(a) Black: voltage driving LED light flash; blue: cumulative density 
function (CDF) of TTL output. The latency from LED activation to 
output TTL is 23.9 $\pm$ 3.9~ms (std. dev). This is consistent
with the 33~ms exposure time. (b) Of the total latency, the image 
processing to extract mean intensity from ten cell-sized regions 
of interest contributes an average of only 0.17~ms; much of the 
ROI feedback latency is a reflection of the frame rate and 
acquisition time.}
\label{fig:video-latency}
\end{figure}

Of the total ROI latency, we can break this down in terms of 
specific steps in the analysis pipeline. Data acquisition, 
region of interest processing, and feedback triggering all 
occur within an average of 1.7~ms after frame 
capture (95\% confidence: $<$11~ms). This variability stems primarily 
from video encoding and storage, where the codec and write buffering 
necessitate increased processing time for a subset of frames. An 
additional  source of variability is processing time, which has a 
smaller impact and is shown in \fref{fig:video-latency}b, relates 
to the number and 
complexity of the defined regions of interest. As the number and size 
of the regions of interest increases, the processing time increases. 
In tests involving ten cell-sized regions of interest, the average 
processing time was 0.17 $\pm$ 0.03~ms (std. dev.).

For audio processing, a similar evaluation can be performed to 
measure average latency for a detection task. Specifically, we 
used a version of the acquisition software extended with a 
trained neural network to identify birdsong syllables 
\cite{Pearre:2017cs}. The detection software was configured to 
process 1.5~ms chunks of audio, sending a rolling window of 
spectral features through the neural network to detect the 
target syllable. Using a recording that includes ground 
truth data, the latency of detection events could be compared 
with actual syllable presentation. \Fref{fig:audio-latency} 
shows the latency for two different TTL output modalities: 
upon detection, the acquisition software could either 
generate a TTL pulse via the connected microcontroller or 
generate a signal on the audio line out (useful for playing 
aversive white noise feedback). Via the audio line out, the 
average latency is 4.2 $\pm$ 2.1~ms. Via the microcontroller,
the average latency is 0.0 $\pm$ 1.8~ms. 

\begin{figure}
\includegraphics[width=\textwidth]{fig4-audio-latency.eps}
\caption[Latency for audio processing]{\textbf{Audio analysis 
produces feedback output in under 5~ms.} The acquisition 
software extended with a neural network based syllable detection 
tool, evaluated on annotated data, allowing evaluation of 
latency between actual syllable presentation and the detection 
TTL pulse. If the TTL pulse was delivered via the attached 
microcontroller, average latency was 0.0 $\pm$ 1.8~ms; if 
delivered via the audio line out, the latency increased to 
4.2 $\pm$ 2.1~ms.}
\label{fig:audio-latency}
\end{figure}

\section{Discussion}

The design of the fiber bundles makes connectorization or 
alignment of two fiber bundles challenging. Instead of an 
attachable recording fiber, we have developed a recording 
configuration that allows chronic recordings in awake 
behaving animals. Based on initial 

Given the challenges of cnonnectorizing or aligning fiber 
bundle ferrules, each with a lattice of thousands of individual 
fibers, we have developed and described an approach to 
perform chronic recordings in awake behaving animals by 
placing the enclosure below a fluorescence microscope. 
Custom software is then used to either manually or 
automatically trigger acquisition during a behavioral 
paradigm, recording video of fluorescence activity and, 
when applicable, synchronized analog data (such as audio).
The software has the ability to do low latency processing 
of both video and analog signals, which can be used to 
control acquisition or provide feedback to the animal.

Using this setup, we were able to chronically monitor 
seven birds and trigger acquisition based off singing. 
Assessment of the fibers after three weeks of acquisition 
show an increase in attenuation, suggesting that even 
with the polyimide reinforcement strip, either the implant 
or recording process results in wear and tear on the 
bundles of optical microfibers, increasing attenuation. 
It may be valuable to collect more example to track 
the decline in attenuation, and potentially improve 
the chronic interface configuration to better protect 
the fibers.

With software capable of near-real-time analysis of
fluorescence activity, the fibers can be used as a 
brain machine interface (BMI) in which an animal controls 
an external device through the measured calcium fluorescence 
measured via the implanted fibers; for example, this could 
be used to allow songbirds direct control over sound 
generation \cite{Clancy:2014gw}. Other applications 
of the software could include closed-loop stimulation experiments 
that seek to optically disrupt specific patterns of 
activity when detected.

In our tests, the latency of triggering off 
fluorescence activity was limited by the sensor 
of the microscope, about 23.9 $\pm$ 3.9~ms 
(\fref{fig:video-latency}). For many experiments, 
this response latency is acceptable. In the songbird, 
for example, the sensory-motor latency from pre-motor 
neural activity in HVC to sensory evaluation of 
the auditory signal processed in the basal ganglia 
is a minimum of 32--50~ms \cite{Andalman:2009bh}. 
Based on these estimates and published accounts 
of conditional feedback experiments in songbirds, 
the time-delay should provide a learnable brain 
machine interface 
\cite{Olveczky:2005bp,Tumer:2007bi,Sakata:2008cm,Sober:2009ci}. 
However it remains to be seen whether the timing 
jitter in the current configuration is acceptable 
for brain machine interface experiments in a 
system as precise as the songbird. For the zebra 
finch, relative jitter between premotor commands 
and auditory feedback is just a few milliseconds. 
For some experiments lower latency and lower 
variability may be desirable, which can be achieved 
through a shorter exposure (higher frame rate). 
To further decrease jitter, the acquisition process 
can potentially be moved to a deterministic 
real-time operating system.

