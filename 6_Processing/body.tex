\chapter{Interface and software for near-real-time processing and feedback}
\label{chapter:processing}

\thispagestyle{myheadings}

% path to figures for this chapter
% must have a trailing slash
\graphicspath{{6_Processing/Figures/}}

\section{Introduction}

Acute recordings from bundles of optical microfibers 
(\cref{chapter:recording}) show the potential of the 
interface to extend optical access to deep brain 
regions. The splaying fibers deliver excitation light,
and collect fluorescent emissions during a cortical 
spreading depolarization, enabling visualizing the 
associated calcium dynamics and propagation of the 
depolarization. Yet a large part of the value proposition 
of the interface is the potential to record fluorescence 
in awake, behaving animals.

In order to achieve such recordings, a new optical 
configuration is required that will allow recording 
animals during a complex behavioral task. This necessitates 
a way to physically manage the fibers such that the bundle is 
protected from physical damage, while maintaining optical 
access to the individual fibers and allowing the animal 
to freely move within their enclosure.

Specifically, we intend to use the fibers to record 
fluorescence activity in singing zebra finches. 
Their ability to freely move about their enclosure 
is crucial; when constrained, the animals will rarely 
sing. To this end, we have designed a recording interface 
that holds a reinforced fiber bundle in a passive 
commutator above the bird's enclosure. As the bird moves 
around its cage, the fiber can rotate below the objective 
of a traditional fluorescence microscope. 

Custom acquisition software controls synchronized acquisition of 
video and analog data; in the case of the zebra finches, we 
use the analog data channels to record audio, 
which is both used for triggering acquisition and for subsequent 
analysis---specifically, to align calcium fluorescence across
song renditions. The software is also capable of near-real-time 
analysis of acquired video frames, extracting fiber 
intensity values, which can be used to trigger 
behavioral feedback or create a basic brain machine interface.

Based on the design configuration used to record from 
seven zebra finches, each for a minimum of a week, we 
describe a hardware and software configuration that 
enables longitudinally recording fluorescence  
from an awake behaving animal.

\section{Methods}

Animal care and experimental procedures were 
approved by the Institutional Animal Care and Use Committee (IACUC) 
of Boston University (protocols 14-028 and 14-029). Fibers were 
implanted in nine zebra finches. Of those, we chronically recorded 
from seven birds via the fiber interface described in the next section; 
two were excluded due to issues with early design iterations, 
where the birds risked getting tangled in the fiber bundle and hence 
were not connected to the microscope.

\subsection{Chronic fiber interface}

To use the bundles of optical microfibers to interface 
with awake behaving animals, we have built and piloted 
a configuration where the animal is housed in an enclosure 
below a traditional fluorescence microscope with the 
ensheathed bundle running from the microscope to 
the animal (\fref{fig:bundle}). The bundle is secured to the animal using 
anchor points around the craniotomy. To avoid physical 
stress on the optical fibers, the bundle is reinforced 
with a strip of polyimide (DuPont Kapton, 0.005'' thick),
prepared as shown in \fref{fig:strip}. The strip 
similarly runs from the skull implant points to the 
fluorescence microscope above the enclosure.

\begin{figure}
\includegraphics[width=\textwidth]{fig1-strip.eps}
\caption[Polyimide to reinforce fiber bundle]{\textbf{A diagram
of a prepared polyimide strip to reinforce a chronic fiber 
bundle implant.} A strip of polyimide is used to absorb physical 
stresses, protecting the bundle of fibers. The fiber bundle is 
threaded through the small holes and the bottom hole (right) is 
used to anchor the polyimide strip to the skull.}
\label{fig:strip}
\end{figure}

The polyimide strip is cut slightly shorter than the fiber 
bundle, so that when the animal moves or twist, the force 
will be primarily exerted on the strip rather than the bundle.
To secure the fiber bundle to the polyimide strip, we 
punched 1.5~mm holes in the strip, and thread the fiber 
bundle through the holes. This ensures that the slack 
in the fiber bundle does not dangle in a way that might 
interfere with the animal's behavior.

Depending on the duration of the recording session, the 
fiber can either be fixed below the fluorescence microscope 
or can be mounted in a low-friction passive commutator,
constructed from precision ball bearings (\fref{fig:commutator}), that allows 
the fiber bundle to rotate as the animal moves. Given the 
flexibility of the fibers, the polyimide reinforcement 
strip is beneficial here to translate the rotational 
torque up to the commutator.

\begin{figure}
\includegraphics[width=\textwidth]{fig2-commutator.jpg}
\caption[Commutator for awake behaving recording]{\textbf{Commutator
built with low-friction, precision ball bearings allows fiber to 
rotate.} A passive commutator constructed with precision ball 
bearings holds the ferrule and polished imaging surface below 
a microscope objective. As the animal moves about the enclosure,
the fiber can freely rotate, while remaining level in the focal 
plane of the microscope.}
\label{fig:commutator}
\end{figure}

To correct for the rotation of the fiber, rotational 
motion correction is applied to the recorded video 
(described in \sref{sec:pipeline}).

In situations requiring longer exposure times, the 
rotation of the fiber may be problematic. We tested a 
further variation that used a servo to apply a rubber ``brake'' pad 
to the passive commutator during recording, temporarily 
preventing rotation.

\subsection{Acquisition hardware}

The ferrule and polished imaging surface were held 
using a passive commutator (\fref{fig:commutator}) 
below a traditional fluorescent microscope (Olympus, 
20$\times$ objective) with a broadband white LED 
(Thorlabs SOLIS-3C) set at 100\% brightness and 
a GFP filter cube (Semrock BrightLine GFP-4050B, 
466/40 excitation, 525/50 emission, 495 dichroic). 
Excitation power from the objective was measured 
at 9.16~mW; given the core size and count, this 
corresponds with coupling approximately 
0.3~\si{\micro\watt} into each 
fiber. Fluorescence activity was recorded either 
with a USB compatible CMOS camera (3rd eye 
electronics, MC900 paired with DM420 frame capture 
device) configured for an exposure of 33~ms resulting 
in a 30 frames per second recording of 640$\times$480 
8 bit pixels or with an 
sCMOS camera (Hamamatsu ORCA-Flash4.0 v2) configured 
for 2$\times$2 binning and an exposure of 50~ms, 
resulting in a 20 frames per second recording of 
1024$\times$1024 16 bit pixels.

When recording via a standard USB CMOS, the custom 
acquisition software described in the next section 
was used to control the camera and acquire video. 
When using the ORCA-Flash sCMOS, the custom 
acquisition software triggered acquisition and 
recorded frame synchronization information, while 
the raw video stream was acquired and written to 
disk by HCImage (Hamamatsu) on a separate computer. 
This allowed isolating the data-intensive (40 
megabytes per second) video acquisition from the 
analog signal acquisition and analysis.

\subsection{Near-real-time software}

To manage and trigger acquisition, and to do near-real-time signal
processing, we created custom image acquisition software that runs
on the macOS operating system (\fref{fig:software}).\footnote{The 
software is open source and accessible at 
\url{https://github.com/gardner-lab/video-capture}}
Leveraging the native AVFoundation framework, the software is 
capable of communicating with a wide range of USB video 
acquisition hardware, including both analog-to-digital frame 
grabbers and cameras. The software can capture video or video 
synchronized with multi-channel analog data (including 
either audio or other analog data). Video and analog data are 
written to disk in MPEG-4 container files with video encoded at 
full resolution either uncompressed, or using the H.264 or lossless 
MJPEG Open DML codecs, and audio encoded using either the AAC codec 
or the Apple Lossless format with a sampling rate of up to 
96~\si{\kilo\hertz}. Simultaneously, the software writes metadata 
including frame timing, region of interest intensity and analysis  
parameters to a text file (CSV format).

\begin{figure}
\includegraphics[width=\textwidth]{fig2-software.png}
\caption[Software for capture and near-real-time analysis]{\textbf{User interface of acquisition
control software capable of near-real-time analysis.} The software 
controls acquisition-related peripherals, including USB camera sensors,
light sources and microcontroller breakout boxes, and manages 
synchronized acquisition of video and analog signals. During 
recording, the software can process video frames, extracting 
multiple annotated regions of interest, and can process analog 
signals, such as performing audio template matching. Analysis 
is used to trigger acquisition or external feedback, acting as 
a basic brain machine interface.}
\label{fig:software}
\end{figure}

The software also communicates with a microcontroller 
(Arduino Uno Rev3 or 2560 Mega) via a USB-to-serial connection. The 
microcontroller allows the software to control additional 
peripherals, including light source brightness, camera parameters 
(such as exposure) and the commutator.

The software is able to perform near real-time analysis on both the 
video frames and the analog input channels---these channels enable
low-latency, video-aligned acquisition of audio, external TTL 
pulses, electrophysiology or behavioral data. 
For example, in experiments where we record fluorescence in songbirds, 
we use the analog channels to acquire audio and trigger acquisition 
based off song detection (by processing audio through a short-time
Fourier transform to calculate the ratio of power in frequency 
bins associated with song [1--10~kHz] to the power of frequency bins outside 
the song range). The microcontroller is used to activate 
the excitation light source, restricting illumination to periods of 
singing and, as a result, minimizing photobleaching. 

During monitoring and acquisition, the software can perform low-latency
computational analysis of both the video frames and analog input. In 
the case of the video, the user interface provides the ability to 
draw regions of interest (ROIs) onto the field of view and processing 
rules in the form of symbolic equations that can be used to trigger 
external feedback (e.g., sound or reward). At the start of
acquisition, the software converts the regions of interest into indices 
corresponding with the ROI masks and parses the symbolic equations 
into an abstract syntax tree (AST) for optimization and evaluation. 
Each frame is read in YCbCr color 
space, providing direct access to the luma component (Y). Using vector
optimized instructions when available, the software accumulates average 
pixel intensity for each region of interest and then evaluates the 
syntax tree and outputs the appropriate feedback value. \Fref{fig:software} 
shows the software configured to compare $\Delta$F values for two 
regions of interest corresponding with two fibers.

Given the broader range of analog analysis and applications, 
the software does not have a graphical 
user interface for defining analog signal analysis, but the code provides clear 
extension points. We have extended and open sourced versions 
of the software that use the analog signals to acquire and process 
audio. Acquired audio is converted to a spectral representation 
through a two-taper short-time Fourier transform (STFT). These 
analysis steps leverage the vDSP Accelerate framework, which 
utilizes vectorized instructions whenever possible to achieve 
low latency processing. We use the spectral representation to 
trigger acquisition (during singing) and to provide behavioral
feedback. The spectral 
representation can serve as an input to a neural network trained 
to precisely identify salient audio (i.e., a syllable or other 
vocalization) \cite{Pearre:2017cs}; upon detection, the software 
can provide aversive feedback (such as burst of white noise) 
to perturb behavior.

To evaluate the software latency, we flash an LED in the 
field of view of the camera, while recording the output trigger 
controlled by the software to measure when the LED intensity is 
detectable. The pulse driving the LED also triggers acquisition on an 
oscilloscope (Tektronix TBS 1022), which is set to averaging mode (128-trigger average) 
in order to collect latency data. In addition, external logging
in the software is used to evaluate processing time for 
specific steps in the analysis pipeline (such as ROI extraction).
Audio processing is evaluated as described in \cite{Pearre:2017cs},
using recordings of songs with a ground truth annotations on the 
second channel. The ground truth signal triggers oscilloscope 
acquisition, again averaging over 128 trials to build a distribution 
of timing performance.

\subsection{Analysis pipeline}
\label{sec:pipeline}

Given the large file size in chronic recordings (200~GB), 
we designed a custom set of tools to aid in file 
processing and analysis.\footnote{The software is open 
source and accessible at 
\url{https://github.com/nathanntg/fiber-pipeline}} 
We created a Python script to 
convert the Hamamatsu CXD file format to raw binary files, 
allowing direct frame indexing. Next, a set of custom 
MATLAB classes allows composing a node based processing 
graph, which allows each frame to be processed in 
isolation, minimizing the amount of memory needed to work 
with the videos.

Our standard pipeline involves iterating over video frames 
twice. During the first pass, raw video frames are 
passed into a Scale-Invariant Feature Transform (SIFT) 
\cite{vedaldi08vlfeat,lowe1999object,Lowe:2004kp} 
motion correction algorithm, capable of correcting 
the rotation of the fiber bundle associated with the 
passive commutator. The transformation matrices 
associated with the motion correction is cached 
for the second pass. The motion corrected frames 
are passed to statistical analysis nodes. The analysis 
nodes accumulate statistics on each pixel, including 
range, mean and percentile intensities. These statistics 
enable identifying high variability pixels (corresponding 
with fibers collecting fluorescence), which can then 
be used to define regions of interest for subsequent 
analysis.

During the second pass, the algorithm again applies the 
motion correction transform to each frame, and extracts 
the mean pixel intensity associated with each region of 
interest. Additional nodes in the software pipeline allow 
optionally applying smoothing, downsampling or background 
subtraction, depending on the desired output.

\section{Results}

The chronic fiber interface configuration and associated video 
acquisition and analysis software were used to record 
from seven zebra finches for 7--23 days each (mean: 
17~days). The recording results are not 
presented here, but the sessions provided an opportunity to 
evaluate and revise the fiber interface for longitudinal 
awake behaving recordings.

\subsection{Chronic fiber interface}

The implanted fibers remained well secured over the 
course of the recording, and the setup provided the 
necessary freedom of motion to allow the birds to 
sing. For all seven animals, we recorded fluorescence 
via the fibers and synchronized audio. Acquisition 
was automatically triggered by singing, and the 
animals performed hundreds of song renditions per day.

After 23 days implanted in an awake behaving animal,
we remeasured the optical attenuation. The optical 
attenuation of the used fibers was measured to be 
6.54~dB (compared with 3.78~dB pre-recording, as 
measured in \sref{sec:methods-fibers}).

\subsection{Analysis performance}

To evaluate the near-real-time analysis and feedback, we recorded 
signals that include a ground truth reference signal, enabling 
evaluation of accuracy and performance.

Region of interest (ROI) video analysis was evaluated by measuring the
time from LED onset to software feedback. The average time to trigger 
a feedback stimulus is 23.9 $\pm$ 3.9~ms (std. dev.), 
including frame exposure, digitization, transmission, data acquisition 
and software processing. This latency reflects the intrinsic 
limitations of a 33~ms exposure time (which accounts for 16.5~ms of 
the delay) as well as the time spent capturing and decoding the video, 
and communicating an output TTL via the microcontroller. 
\Fref{fig:video-latency} depicts video analysis latency.

\begin{figure}
\includegraphics[width=\textwidth]{fig3-video-latency.eps}
\caption[Latency for video processing]{\textbf{Video analysis 
produces feedback in 24~ms, including exposure time.} To measure 
the latency of the video analysis functionality in the 
acquisition software, an LED was flashed in the field of view,
while timing the delay until detection of the output TTL indicating detection. 
(a) Black: voltage driving LED light flash; blue: cumulative density 
function (CDF) of TTL output. The latency from LED activation to 
output TTL is 23.9 $\pm$ 3.9~ms (std. dev). This is consistent
with the 33~ms exposure time. (b) Of the total latency, the image 
processing to extract mean intensity from ten cell-sized regions 
of interest contributes an average of only 0.17~ms; much of the 
ROI feedback latency is a reflection of the frame rate and 
acquisition time.}
\label{fig:video-latency}
\end{figure}

Of the total ROI latency, we can break this down in terms of 
specific steps in the analysis pipeline. Data acquisition, 
region of interest processing, and feedback triggering all 
occur within an average of 1.7~ms after frame 
capture (95\% confidence: $<$11~ms). This variability stems primarily 
from video encoding and storage, where the codec and write buffering 
necessitate increased processing time for a subset of frames. An 
additional  source of variability is processing time, which has a 
smaller impact and is shown in \fref{fig:video-latency}b, relates 
to the number and 
complexity of the defined regions of interest. As the number and size 
of the regions of interest increases, the processing time increases. 
In tests involving ten cell-sized regions of interest, the average 
processing time was 0.17 $\pm$ 0.03~ms (std. dev.).

For audio processing, a similar evaluation can be performed to 
measure average latency for a detection task. Specifically, we 
used a version of the acquisition software extended with a 
trained neural network to identify birdsong syllables 
\cite{Pearre:2017cs}. The detection software was configured to 
process 1.5~ms chunks of audio, sending a rolling window of 
spectral features through the neural network to detect the 
target syllable. Using a recording that includes ground 
truth data, the latency of detection events could be compared 
with actual syllable presentation. \Fref{fig:audio-latency} 
shows the latency for two different TTL output modalities: 
upon detection, the acquisition software could either 
generate a TTL pulse via the connected microcontroller or 
generate a signal on the audio line out (useful for playing 
aversive white noise feedback). Via the audio line out, the 
average latency is 4.2 $\pm$ 2.1~ms. Via the microcontroller,
the average latency is 0.0 $\pm$ 1.8~ms. 

\begin{figure}
\includegraphics[width=\textwidth]{fig4-audio-latency.eps}
\caption[Latency for audio processing]{\textbf{Audio analysis 
produces feedback in under 5~ms.} The acquisition 
software extended with a neural network based syllable detection 
tool, was evaluated on annotated data, allowing measurement of the 
latency between actual syllable presentation and the detection 
TTL pulse. If the TTL pulse was delivered via the attached 
microcontroller, average latency was 0.0 $\pm$ 1.8~ms; if 
delivered via the audio line out, the latency increased to 
4.2 $\pm$ 2.1~ms.}
\label{fig:audio-latency}
\end{figure}

\section{Discussion}

Given the challenges of connectorizing or aligning fiber 
bundle ferrules, each with a lattice of thousands of individual 
fibers, we have developed and described an approach to 
perform chronic recordings in awake behaving animals by 
placing the enclosure below a fluorescence microscope. 
Custom software is then used to either manually or 
automatically trigger acquisition during a behavioral 
paradigm, recording video of fluorescence activity and, 
when applicable, synchronized analog data (such as audio).
The software has the ability to do low latency processing 
of both video and analog signals, which can be used to 
control acquisition or provide feedback to the animal.

Using this setup, we were able to chronically monitor 
seven birds and trigger acquisition based off singing. 
Assessment of the fibers after three weeks of acquisition 
show an increase in attenuation, suggesting that even 
with the polyimide reinforcement strip, either the implant 
or recording process results in wear and tear on the 
bundles of optical microfibers, increasing attenuation. 
It may be valuable to collect more measurements to track 
the decline in attenuation, and potentially improve 
the chronic interface configuration to better protect 
the fibers.

With software capable of near-real-time analysis of
fluorescence activity, the fibers can be used as a 
brain machine interface (BMI) in which an animal controls 
an external device through the calcium fluorescence 
measured via the implanted fibers; for example, this could 
be used to allow songbirds direct control over sound 
generation \cite{Clancy:2014gw}. Other applications 
of the software could include closed-loop stimulation experiments 
that seek to optically disrupt specific patterns of 
activity when detected.

In our tests, the latency of triggering off 
fluorescence activity was limited by the sensor 
of the camera, about 23.9 $\pm$ 3.9~ms 
(\fref{fig:video-latency}). For many experiments, 
this response latency is acceptable. In the songbird, 
for example, the sensory-motor latency from pre-motor 
neural activity in HVC to sensory evaluation of 
the auditory signal processed in the basal ganglia 
is a minimum of 32--50~ms \cite{Andalman:2009bh}. 
Based on these estimates and published accounts 
of conditional feedback experiments in songbirds, 
the time-delay should provide a learnable brain 
machine interface 
\cite{Olveczky:2005bp,Tumer:2007bi,Sakata:2008cm,Sober:2009ci}. 
However it remains to be seen whether the timing 
jitter in the current configuration is acceptable 
for brain machine interface experiments in a 
system as precise as the songbird. For the zebra 
finch, relative jitter between premotor commands 
and auditory feedback is just a few milliseconds. 
For some experiments, lower latency and lower 
variability may be desirable, which can be achieved 
through a shorter exposure (higher frame rate). 
To further decrease jitter, the acquisition process 
can potentially be moved to a deterministic 
real-time operating system.

